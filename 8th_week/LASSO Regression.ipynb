{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"colab":{"name":"LASSO Regression.ipynb","version":"0.3.2","provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"vmyeaIu6GEUO","colab_type":"text"},"source":["# LASSO Regression\n","In this notebook we have code to compare the performance of LASSO Regression VS Linear Regression with no regularization"]},{"cell_type":"code","metadata":{"id":"F0m79OCbGEUQ","colab_type":"code","colab":{}},"source":["import matplotlib.pyplot as plt\n","import numpy as np \n","import pandas as pd\n","import matplotlib\n","from sklearn.cross_validation import train_test_split\n","from sklearn.linear_model import LinearRegression\n","from sklearn.linear_model import Lasso\n","from sklearn.model_selection import GridSearchCV\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"xoSiywsWGEUS","colab_type":"code","colab":{}},"source":["#Simple Linear Regression\n","lr = LinearRegression()\n","lr.fit(X_train, y_train)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7kMsILGDGEUU","colab_type":"code","colab":{}},"source":["lasso001 = Lasso(alpha=0.01)\n","lasso001.fit(X_train,y_train)\n","\n","\n","lasso00001 = Lasso(alpha=0.0001)\n","lasso00001.fit(X_train,y_train)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9JhkNqmkGEUW","colab_type":"code","colab":{}},"source":["#If we want to search for best parameters we can buid a GridSearchCV \n","lasso = Lasso()\n","parameters = {'alpha':[1e-15, 1e-10, 1e-8, 1e-4, 1e-3, 1e-2, 1, 5, 10, 20]}\n","lasso_regressor = GridSearchCV(lasso, parameters, scoring='neg_mean_squared_error', cv=10)\n","lasso_regressor.fit(X_train, y_train)\n","\n","print(lasso_regressor.best_params_)\n","print(lasso_regressor.best_score_)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QRbolWSKGEUY","colab_type":"code","colab":{}},"source":["train_score=lr.score(X_train, y_train)\n","test_score=lr.score(X_test, y_test)\n","lasso_train_score001=lasso001.score(X_train,y_train)\n","lasso_test_score001=lasso001.score(X_test,y_test)\n","lasso_train_score00001=lasso00001.score(X_train,y_train)\n","lasso_test_score00001=lasso00001.score(X_test,y_test)\n","\n","print \"LR training score:\", lr_train_score \n","print \"LR test score: \", lr_test_score\n","print \"training score for alpha=0.01:\", train_score001 \n","print \"test score for alpha =0.01: \", test_score001\n","print \"training score for alpha=0.0001:\", train_score00001 \n","print \"test score for alpha =0.0001: \", test_score00001\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"DceQMcm4GEUa","colab_type":"code","colab":{}},"source":["#If we want to check number of features used\n","coeff_used001 = np.sum(lasso001.coef_!=0)\n","coeff_used00001 = np.sum(lasso00001.coef_!=0)\n","\n","print \"number of features used: for alpha =0.01:\", coeff_used001\n","print \"number of features used: for alpha =0.0001:\", coeff_used00001\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7r8Sj9y8GEUc","colab_type":"text"},"source":["In the case of logistic regression is easier because there is a parameter for penality"]},{"cell_type":"code","metadata":{"id":"LBfLMN6GGEUd","colab_type":"code","colab":{}},"source":["from sklearn.linear_model import LogisticRegression\n","from sklearn.datasets import load_iris\n","X, y = load_iris(return_X_y=True)\n","log = LogisticRegression(penalty='l1', solver='liblinear')\n","log.fit(X, y)"],"execution_count":0,"outputs":[]}]}