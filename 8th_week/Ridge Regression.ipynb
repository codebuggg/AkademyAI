{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"colab":{"name":"Ridge Regression.ipynb","version":"0.3.2","provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"ZR3SI3zEFTQE","colab_type":"text"},"source":["# Ridge Regression\n","In this notebook we have code to compare the performance of Ridge Regression VS Linear Regression with no regularization"]},{"cell_type":"code","metadata":{"id":"oqPVfMC0FTQF","colab_type":"code","colab":{}},"source":["import matplotlib.pyplot as plt\n","import numpy as np \n","import pandas as pd\n","import matplotlib\n","from sklearn.cross_validation import train_test_split\n","from sklearn.linear_model import LinearRegression\n","from sklearn.linear_model import Ridge\n","from sklearn.model_selection import GridSearchCV\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"pDnx8fqbFTQI","colab_type":"code","colab":{}},"source":["#Simple Linear Regression\n","lr = LinearRegression()\n","lr.fit(X_train, y_train)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4WrRzKGiFTQK","colab_type":"code","colab":{}},"source":["#alpha (also named lambda) is the penalization term\n","# higher the alpha value, more restriction on the coefficients\n","rr = Ridge(alpha=0.01) \n","rr.fit(X_train, y_train)\n","\n","rr100 = Ridge(alpha=100) #  comparison with alpha value\n","rr100.fit(X_train, y_train)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"oWbJmxYVFTQM","colab_type":"code","colab":{}},"source":["#If we want to search for best parameters we can buid a GridSearchCV \n","ridge = Ridge()\n","parameters = {'alpha':[1e-15, 1e-10, 1e-8, 1e-4, 1e-3, 1e-2, 1, 5, 10, 20]}\n","ridge_regressor = GridSearchCV(ridge, parameters, scoring='neg_mean_squared_error', cv=10)\n","ridge_regressor.fit(X_train, y_train)\n","\n","print(ridge_regressor.best_params_)\n","print(ridge_regressor.best_score_)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"odqYFrsEFTQO","colab_type":"code","colab":{}},"source":["#Compare scores\n","train_score=lr.score(X_train, y_train)\n","test_score=lr.score(X_test, y_test)\n","Ridge_train_score = rr.score(X_train,y_train)\n","Ridge_test_score = rr.score(X_test, y_test)\n","Ridge_train_score100 = rr100.score(X_train,y_train)\n","Ridge_test_score100 = rr100.score(X_test, y_test)\n","\n","print \"linear regression train score:\", train_score\n","print \"linear regression test score:\", test_score\n","print \"ridge regression train score low alpha:\", Ridge_train_score\n","print \"ridge regression test score low alpha:\", Ridge_test_score\n","print \"ridge regression train score high alpha:\", Ridge_train_score100\n","print \"ridge regression test score high alpha:\", Ridge_test_score100"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yDygtyK5FTQQ","colab_type":"code","colab":{}},"source":["#Use to plot the previous results\n","plt.plot(rr.coef_,alpha=0.7,linestyle='none',marker='*',markersize=5,color='red',label=r'Ridge; $\\alpha = 0.01$',zorder=7) # zorder for ordering the markers\n","plt.plot(rr100.coef_,alpha=0.5,linestyle='none',marker='d',markersize=6,color='blue',label=r'Ridge; $\\alpha = 100$') # alpha here is for transparency\n","plt.plot(lr.coef_,alpha=0.4,linestyle='none',marker='o',markersize=7,color='green',label='Linear Regression')\n","plt.xlabel('Coefficient Index',fontsize=16)\n","plt.ylabel('Coefficient Magnitude',fontsize=16)\n","plt.legend(fontsize=13,loc=4)\n","plt.show()"],"execution_count":0,"outputs":[]}]}