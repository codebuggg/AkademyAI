{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--NAVIGATION-->\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/bpesquet/machine-learning-katas/blob/master/classic-datasets/Iris.ipynb\"><img align=\"left\" src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open in Colab\" title=\"Open in Google Colaboratory\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Livecoding: Iris Dataset\n",
    "\n",
    "| Learning type | Activity type | Objective |\n",
    "| - | - | - |\n",
    "| Supervised | Multiclass classification | Identify a flower's class |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions with NBGrader removed\n",
    "\n",
    "Complete the cells beginning with `# YOUR CODE HERE` and run the subsequent cells to check your code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About the dataset\n",
    "\n",
    "[Iris](https://archive.ics.uci.edu/ml/datasets/iris) is a well-known multiclass dataset. It contains 3 classes of flowers with 50 examples each. There are a total of 4 features for each flower.\n",
    "\n",
    "![](./classic-datasets/images/Iris-versicolor-21_1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Package setups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\USER\\\\.conda\\\\envs\\\\AkademyAI\\\\python.exe'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#to debug package errors\n",
    "import sys\n",
    "sys.path\n",
    "sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import needed packages\n",
    "# You may add or remove packages should you need them\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "#import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn import model_selection\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import KFold, LeaveOneOut, cross_val_score\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(0)\n",
    "\n",
    "# Display plots inline and change plot resolution to retina\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "# Set Seaborn aesthetic parameters to defaults\n",
    "#sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "      <th>target</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>5.8</td>\n",
       "      <td>2.8</td>\n",
       "      <td>5.1</td>\n",
       "      <td>2.4</td>\n",
       "      <td>2</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>6.0</td>\n",
       "      <td>2.2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>5.5</td>\n",
       "      <td>4.2</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>7.3</td>\n",
       "      <td>2.9</td>\n",
       "      <td>6.3</td>\n",
       "      <td>1.8</td>\n",
       "      <td>2</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>6.3</td>\n",
       "      <td>3.3</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.1</td>\n",
       "      <td>4.7</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>6.8</td>\n",
       "      <td>2.8</td>\n",
       "      <td>4.8</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>6.1</td>\n",
       "      <td>2.8</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
       "114                5.8               2.8                5.1               2.4   \n",
       "62                 6.0               2.2                4.0               1.0   \n",
       "33                 5.5               4.2                1.4               0.2   \n",
       "107                7.3               2.9                6.3               1.8   \n",
       "7                  5.0               3.4                1.5               0.2   \n",
       "100                6.3               3.3                6.0               2.5   \n",
       "40                 5.0               3.5                1.3               0.3   \n",
       "86                 6.7               3.1                4.7               1.5   \n",
       "76                 6.8               2.8                4.8               1.4   \n",
       "71                 6.1               2.8                4.0               1.3   \n",
       "\n",
       "     target       class  \n",
       "114       2   virginica  \n",
       "62        1  versicolor  \n",
       "33        0      setosa  \n",
       "107       2   virginica  \n",
       "7         0      setosa  \n",
       "100       2   virginica  \n",
       "40        0      setosa  \n",
       "86        1  versicolor  \n",
       "76        1  versicolor  \n",
       "71        1  versicolor  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the Iris dataset included with scikit-learn\n",
    "dataset = load_iris()\n",
    "\n",
    "# Put data in a pandas DataFrame\n",
    "df_iris = pd.DataFrame(dataset.data, columns=dataset.feature_names)\n",
    "# Add target and class to DataFrame\n",
    "df_iris['target'] = dataset.target\n",
    "df_iris['class'] = dataset.target_names[dataset.target]\n",
    "# Show 10 random samples\n",
    "df_iris.sample(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question\n",
    "Find the X and y values we're looking for. Notice that y is categorical and thus, we could **one-hot encode it** if we are looking at **class** or we can just pick **target**. In order to one hot encode we have  to re-shape `y` it using the **.get_dummies** function. \n",
    "\n",
    "## For the purpose of this exercise, do not use hot encoding, go only for target but think about if you have to drop it somewhere or not..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>setosa</th>\n",
       "      <th>versicolor</th>\n",
       "      <th>virginica</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   setosa  versicolor  virginica\n",
       "0       1           0          0\n",
       "1       1           0          0\n",
       "2       1           0          0\n",
       "3       1           0          0\n",
       "4       1           0          0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df_iris.drop([\"target\", \"class\"], axis=1)\n",
    "y = df_iris[\"target\"]\n",
    "y = pd.get_dummies(df_iris['class'])\n",
    "#x.head()\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Normalize the data in X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.80377277 0.55160877 0.22064351 0.0315205 ]\n",
      " [0.82813287 0.50702013 0.23660939 0.03380134]\n",
      " [0.80533308 0.54831188 0.2227517  0.03426949]\n",
      " [0.80003025 0.53915082 0.26087943 0.03478392]\n",
      " [0.790965   0.5694948  0.2214702  0.0316386 ]\n",
      " [0.78417499 0.5663486  0.2468699  0.05808704]\n",
      " [0.78010936 0.57660257 0.23742459 0.0508767 ]\n",
      " [0.80218492 0.54548574 0.24065548 0.0320874 ]\n",
      " [0.80642366 0.5315065  0.25658935 0.03665562]\n",
      " [0.81803119 0.51752994 0.25041771 0.01669451]\n",
      " [0.80373519 0.55070744 0.22325977 0.02976797]\n",
      " [0.786991   0.55745196 0.26233033 0.03279129]\n",
      " [0.82307218 0.51442011 0.24006272 0.01714734]\n",
      " [0.8025126  0.55989251 0.20529392 0.01866308]\n",
      " [0.81120865 0.55945424 0.16783627 0.02797271]\n",
      " [0.77381111 0.59732787 0.2036345  0.05430253]\n",
      " [0.79428944 0.57365349 0.19121783 0.05883625]\n",
      " [0.80327412 0.55126656 0.22050662 0.04725142]\n",
      " [0.8068282  0.53788547 0.24063297 0.04246464]\n",
      " [0.77964883 0.58091482 0.22930848 0.0458617 ]\n",
      " [0.8173379  0.51462016 0.25731008 0.03027177]\n",
      " [0.78591858 0.57017622 0.23115252 0.06164067]\n",
      " [0.77577075 0.60712493 0.16864581 0.03372916]\n",
      " [0.80597792 0.52151512 0.26865931 0.07901744]\n",
      " [0.776114   0.54974742 0.30721179 0.03233808]\n",
      " [0.82647451 0.4958847  0.26447184 0.03305898]\n",
      " [0.79778206 0.5424918  0.25529026 0.06382256]\n",
      " [0.80641965 0.54278246 0.23262105 0.03101614]\n",
      " [0.81609427 0.5336001  0.21971769 0.03138824]\n",
      " [0.79524064 0.54144043 0.27072022 0.03384003]\n",
      " [0.80846584 0.52213419 0.26948861 0.03368608]\n",
      " [0.82225028 0.51771314 0.22840286 0.06090743]\n",
      " [0.76578311 0.60379053 0.22089897 0.0147266 ]\n",
      " [0.77867447 0.59462414 0.19820805 0.02831544]\n",
      " [0.81768942 0.51731371 0.25031309 0.03337508]\n",
      " [0.82512295 0.52807869 0.19802951 0.03300492]\n",
      " [0.82699754 0.52627116 0.19547215 0.03007264]\n",
      " [0.78523221 0.5769053  0.22435206 0.01602515]\n",
      " [0.80212413 0.54690282 0.23699122 0.03646019]\n",
      " [0.80779568 0.53853046 0.23758697 0.03167826]\n",
      " [0.80033301 0.56023311 0.20808658 0.04801998]\n",
      " [0.86093857 0.44003527 0.24871559 0.0573959 ]\n",
      " [0.78609038 0.57170209 0.23225397 0.03573138]\n",
      " [0.78889479 0.55222635 0.25244633 0.09466737]\n",
      " [0.76693897 0.57144472 0.28572236 0.06015208]\n",
      " [0.82210585 0.51381615 0.23978087 0.05138162]\n",
      " [0.77729093 0.57915795 0.24385598 0.030482  ]\n",
      " [0.79594782 0.55370283 0.24224499 0.03460643]\n",
      " [0.79837025 0.55735281 0.22595384 0.03012718]\n",
      " [0.81228363 0.5361072  0.22743942 0.03249135]\n",
      " [0.76701103 0.35063361 0.51499312 0.15340221]\n",
      " [0.74549757 0.37274878 0.52417798 0.17472599]\n",
      " [0.75519285 0.33928954 0.53629637 0.16417236]\n",
      " [0.75384916 0.31524601 0.54825394 0.17818253]\n",
      " [0.7581754  0.32659863 0.5365549  0.17496355]\n",
      " [0.72232962 0.35482858 0.57026022 0.16474184]\n",
      " [0.72634846 0.38046824 0.54187901 0.18446945]\n",
      " [0.75916547 0.37183615 0.51127471 0.15493173]\n",
      " [0.76301853 0.33526572 0.53180079 0.15029153]\n",
      " [0.72460233 0.37623583 0.54345175 0.19508524]\n",
      " [0.76923077 0.30769231 0.53846154 0.15384615]\n",
      " [0.73923462 0.37588201 0.52623481 0.187941  ]\n",
      " [0.78892752 0.28927343 0.52595168 0.13148792]\n",
      " [0.73081412 0.34743622 0.56308629 0.16772783]\n",
      " [0.75911707 0.3931142  0.48800383 0.17622361]\n",
      " [0.76945444 0.35601624 0.50531337 0.16078153]\n",
      " [0.70631892 0.37838513 0.5675777  0.18919257]\n",
      " [0.75676497 0.35228714 0.53495455 0.13047672]\n",
      " [0.76444238 0.27125375 0.55483721 0.18494574]\n",
      " [0.76185188 0.34011245 0.53057542 0.14964948]\n",
      " [0.6985796  0.37889063 0.56833595 0.21312598]\n",
      " [0.77011854 0.35349703 0.50499576 0.16412362]\n",
      " [0.74143307 0.29421947 0.57667016 0.17653168]\n",
      " [0.73659895 0.33811099 0.56754345 0.14490471]\n",
      " [0.76741698 0.34773582 0.51560829 0.15588157]\n",
      " [0.76785726 0.34902603 0.51190484 0.16287881]\n",
      " [0.76467269 0.31486523 0.53976896 0.15743261]\n",
      " [0.74088576 0.33173989 0.55289982 0.18798594]\n",
      " [0.73350949 0.35452959 0.55013212 0.18337737]\n",
      " [0.78667474 0.35883409 0.48304589 0.13801311]\n",
      " [0.76521855 0.33391355 0.52869645 0.15304371]\n",
      " [0.77242925 0.33706004 0.51963422 0.14044168]\n",
      " [0.76434981 0.35581802 0.51395936 0.15814134]\n",
      " [0.70779525 0.31850786 0.60162596 0.1887454 ]\n",
      " [0.69333409 0.38518561 0.57777841 0.1925928 ]\n",
      " [0.71524936 0.40530797 0.53643702 0.19073316]\n",
      " [0.75457341 0.34913098 0.52932761 0.16893434]\n",
      " [0.77530021 0.28304611 0.54147951 0.15998258]\n",
      " [0.72992443 0.39103094 0.53440896 0.16944674]\n",
      " [0.74714194 0.33960997 0.54337595 0.17659719]\n",
      " [0.72337118 0.34195729 0.57869695 0.15782644]\n",
      " [0.73260391 0.36029701 0.55245541 0.1681386 ]\n",
      " [0.76262994 0.34186859 0.52595168 0.1577855 ]\n",
      " [0.76986879 0.35413965 0.5081134  0.15397376]\n",
      " [0.73544284 0.35458851 0.55158213 0.1707278 ]\n",
      " [0.73239618 0.38547167 0.53966034 0.15418867]\n",
      " [0.73446047 0.37367287 0.5411814  0.16750853]\n",
      " [0.75728103 0.3542121  0.52521104 0.15878473]\n",
      " [0.78258054 0.38361791 0.4603415  0.16879188]\n",
      " [0.7431482  0.36505526 0.5345452  0.16948994]\n",
      " [0.65387747 0.34250725 0.62274045 0.25947519]\n",
      " [0.69052512 0.32145135 0.60718588 0.22620651]\n",
      " [0.71491405 0.30207636 0.59408351 0.21145345]\n",
      " [0.69276796 0.31889319 0.61579374 0.1979337 ]\n",
      " [0.68619022 0.31670318 0.61229281 0.232249  ]\n",
      " [0.70953708 0.28008043 0.61617694 0.1960563 ]\n",
      " [0.67054118 0.34211284 0.61580312 0.23263673]\n",
      " [0.71366557 0.28351098 0.61590317 0.17597233]\n",
      " [0.71414125 0.26647062 0.61821183 0.19185884]\n",
      " [0.69198788 0.34599394 0.58626751 0.24027357]\n",
      " [0.71562645 0.3523084  0.56149152 0.22019275]\n",
      " [0.71576546 0.30196356 0.59274328 0.21249287]\n",
      " [0.71718148 0.31640359 0.58007326 0.22148252]\n",
      " [0.6925518  0.30375079 0.60750157 0.24300063]\n",
      " [0.67767924 0.32715549 0.59589036 0.28041899]\n",
      " [0.69589887 0.34794944 0.57629125 0.25008866]\n",
      " [0.70610474 0.3258945  0.59747324 0.1955367 ]\n",
      " [0.69299099 0.34199555 0.60299216 0.19799743]\n",
      " [0.70600618 0.2383917  0.63265489 0.21088496]\n",
      " [0.72712585 0.26661281 0.60593821 0.18178146]\n",
      " [0.70558934 0.32722984 0.58287815 0.23519645]\n",
      " [0.68307923 0.34153961 0.59769433 0.24395687]\n",
      " [0.71486543 0.25995106 0.62202576 0.18567933]\n",
      " [0.73122464 0.31338199 0.56873028 0.20892133]\n",
      " [0.69595601 0.3427843  0.59208198 0.21813547]\n",
      " [0.71529453 0.31790868 0.59607878 0.17882363]\n",
      " [0.72785195 0.32870733 0.56349829 0.21131186]\n",
      " [0.71171214 0.35002236 0.57170319 0.21001342]\n",
      " [0.69594002 0.30447376 0.60894751 0.22835532]\n",
      " [0.73089855 0.30454106 0.58877939 0.1624219 ]\n",
      " [0.72766159 0.27533141 0.59982915 0.18683203]\n",
      " [0.71578999 0.34430405 0.5798805  0.18121266]\n",
      " [0.69417747 0.30370264 0.60740528 0.2386235 ]\n",
      " [0.72366005 0.32162669 0.58582004 0.17230001]\n",
      " [0.69385414 0.29574111 0.63698085 0.15924521]\n",
      " [0.73154399 0.28501714 0.57953485 0.21851314]\n",
      " [0.67017484 0.36168166 0.59571097 0.2553047 ]\n",
      " [0.69804799 0.338117   0.59988499 0.196326  ]\n",
      " [0.71066905 0.35533453 0.56853524 0.21320072]\n",
      " [0.72415258 0.32534391 0.56672811 0.22039426]\n",
      " [0.69997037 0.32386689 0.58504986 0.25073566]\n",
      " [0.73337886 0.32948905 0.54206264 0.24445962]\n",
      " [0.69052512 0.32145135 0.60718588 0.22620651]\n",
      " [0.69193502 0.32561648 0.60035539 0.23403685]\n",
      " [0.68914871 0.33943145 0.58629069 0.25714504]\n",
      " [0.72155725 0.32308533 0.56001458 0.24769876]\n",
      " [0.72965359 0.28954508 0.57909015 0.22005426]\n",
      " [0.71653899 0.3307103  0.57323119 0.22047353]\n",
      " [0.67467072 0.36998072 0.58761643 0.25028107]\n",
      " [0.69025916 0.35097923 0.5966647  0.21058754]]\n"
     ]
    }
   ],
   "source": [
    "#scaler = StandardScaler().fit(X)\n",
    "scaler = preprocessing.normalize(X)\n",
    "print(scaler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question\n",
    "\n",
    "Is it better to store it on numpy or in an updated pandas dataframe? Numpy is more efficient while pandas is more visual. **pick whatever your prefer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.803773</td>\n",
       "      <td>0.551609</td>\n",
       "      <td>0.220644</td>\n",
       "      <td>0.031521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.828133</td>\n",
       "      <td>0.507020</td>\n",
       "      <td>0.236609</td>\n",
       "      <td>0.033801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.805333</td>\n",
       "      <td>0.548312</td>\n",
       "      <td>0.222752</td>\n",
       "      <td>0.034269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.800030</td>\n",
       "      <td>0.539151</td>\n",
       "      <td>0.260879</td>\n",
       "      <td>0.034784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.790965</td>\n",
       "      <td>0.569495</td>\n",
       "      <td>0.221470</td>\n",
       "      <td>0.031639</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3\n",
       "0  0.803773  0.551609  0.220644  0.031521\n",
       "1  0.828133  0.507020  0.236609  0.033801\n",
       "2  0.805333  0.548312  0.222752  0.034269\n",
       "3  0.800030  0.539151  0.260879  0.034784\n",
       "4  0.790965  0.569495  0.221470  0.031639"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = pd.DataFrame(scaler)\n",
    "#print(X)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Train / Test Split\n",
    "\n",
    "Store training input data in a variable named `x_train` and training targets in a variable named `y_train` with an **80/20 train/test split**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (120, 4). y_train: (120, 3)\n",
      "Labels:      setosa  versicolor  virginica\n",
      "22        1           0          0\n",
      "15        1           0          0\n",
      "65        0           1          0\n",
      "11        1           0          0\n",
      "42        1           0          0\n",
      "146       0           0          1\n",
      "51        0           1          0\n",
      "27        1           0          0\n",
      "4         1           0          0\n",
      "32        1           0          0\n",
      "142       0           0          1\n",
      "85        0           1          0\n",
      "86        0           1          0\n",
      "16        1           0          0\n",
      "10        1           0          0\n",
      "81        0           1          0\n",
      "133       0           0          1\n",
      "137       0           0          1\n",
      "75        0           1          0\n",
      "109       0           0          1\n",
      "96        0           1          0\n",
      "105       0           0          1\n",
      "66        0           1          0\n",
      "0         1           0          0\n",
      "122       0           0          1\n",
      "67        0           1          0\n",
      "28        1           0          0\n",
      "40        1           0          0\n",
      "44        1           0          0\n",
      "60        0           1          0\n",
      "..      ...         ...        ...\n",
      "91        0           1          0\n",
      "41        1           0          0\n",
      "58        0           1          0\n",
      "90        0           1          0\n",
      "48        1           0          0\n",
      "88        0           1          0\n",
      "107       0           0          1\n",
      "124       0           0          1\n",
      "21        1           0          0\n",
      "57        0           1          0\n",
      "144       0           0          1\n",
      "129       0           0          1\n",
      "37        1           0          0\n",
      "140       0           0          1\n",
      "1         1           0          0\n",
      "52        0           1          0\n",
      "130       0           0          1\n",
      "103       0           0          1\n",
      "99        0           1          0\n",
      "116       0           0          1\n",
      "87        0           1          0\n",
      "74        0           1          0\n",
      "121       0           0          1\n",
      "149       0           0          1\n",
      "20        1           0          0\n",
      "71        0           1          0\n",
      "106       0           0          1\n",
      "14        1           0          0\n",
      "92        0           1          0\n",
      "102       0           0          1\n",
      "\n",
      "[120 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "print(f'X_train: {X_train.shape}. y_train: {y_train.shape}')\n",
    "print(f'Labels: {y_train}')\n",
    "assert X_train.shape == (120,4)\n",
    "# only if we hot encode\n",
    "# assert y_train.shape == (120,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of observations in the training data: 120\n",
      "Number of observations in the test data: 30\n"
     ]
    }
   ],
   "source": [
    "# Show the number of observations for the test and training dataframes\n",
    "print('Number of observations in the training data:', len(X_train))\n",
    "print('Number of observations in the test data:',len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Training a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a model on the data to obtain a training accuracy > 93%. Store the training history in a variable named `history`.\n",
    "\n",
    "For example, we can use **RandomForestClassifier** model to classify Irises. N_estimators increases model accuracy. Other options include:\n",
    "- K-Nearest Neighbors\n",
    "- Support Vector Machines\n",
    "- Decision Tree Classifiers\n",
    "- Naive Bayes\n",
    "- Linear Discriminant Analysis\n",
    "- Logistic Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=-1, normalize=False)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "predictor = LinearRegression(n_jobs =-1)\n",
    "model = predictor.fit(X_train, y_train)\n",
    "print(model)\n",
    "#clf = RandomForestClassifier(n_estimators = 10000).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can measure the accuracy with our **test set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\.conda\\envs\\AkademyAI\\lib\\site-packages\\sklearn\\base.py:420: FutureWarning: The default value of multioutput (not exposed in score method) will change from 'variance_weighted' to 'uniform_average' in 0.23 to keep consistent with 'metrics.r2_score'. To specify the default value manually and avoid the warning, please either call 'metrics.r2_score' directly or make a custom scorer with 'metrics.make_scorer' (the built-in scorer 'r2' uses multioutput='uniform_average').\n",
      "  \"multioutput='uniform_average').\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7606825000947363"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can predict what a new input would look like..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-25.19748636, 123.98502015, -97.78753379]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict([[4,2,1,2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "can you build a table with each predicted value and its corresponding category in string?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
