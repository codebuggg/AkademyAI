{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline --no-import-all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Function minimization\n",
    "\n",
    "At this week's lecture we discussed how learning a set of parameters can be treated as the task of minimizing an error function. Scipy privides a number of ways of finding minima of functions. We'll test one of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from scipy.optimize import fmin_bfgs, fmin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `fmin_bfgs` uses the method called \n",
    "[BFGS](https://en.wikipedia.org/wiki/Broyden%E2%80%93Fletcher%E2%80%93Goldfarb%E2%80%93Shanno_algorithm).\n",
    "We need to give it the following arguments:\n",
    "- f: the function to minimize\n",
    "- x0: the initial guess of the argument with respect to which we're minimizing\n",
    "- fprime: the first derivative of f. If we omit it, `fmin_bfgs` will use a numerical approximatiom\n",
    "\n",
    "For example, consider the following polynomial function: $f(x) = x^4 - 10x^3 + x^2 + x - 4$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return x**4 - 10*x**3 + x**2 + x -4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f6113c3bba8>]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6113cc36d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = numpy.linspace(-2,10,1000)\n",
    "pylab.plot(x,f(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find the minimum, using 0 as a starting point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: -4.093250\n",
      "         Iterations: 5\n",
      "         Function evaluations: 24\n",
      "         Gradient evaluations: 8\n",
      "[-0.15101746]\n"
     ]
    }
   ],
   "source": [
    "print(fmin_bfgs(f, x0=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can suppress the messages using disp=False:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.15101746]\n"
     ]
    }
   ],
   "source": [
    "print(fmin_bfgs(f, x0=0, disp=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4.1\n",
    "Looks like we got stuck in a local miniumum. Print the values of the x where f(x) has a minimum findable from the following starting points: -2, 0, 2, 6, 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting at -2 found minimum at [-0.15101775]\n",
      "Starting at 0 found minimum at [-0.15101746]\n",
      "Starting at 2 found minimum at [7.42815758]\n",
      "Starting at 6 found minimum at [7.42815774]\n",
      "Starting at 10 found minimum at [7.4281578]\n"
     ]
    }
   ],
   "source": [
    "# .............................\n",
    "for x0 in [-2, 0, 2, 6, 10]:\n",
    "    x_min = fmin_bfgs(f, x0=x0, disp=False)\n",
    "    print(\"Starting at {} found minimum at {}\".format(x0,x_min))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We can also use `fmin_bfgs` to minimize functions which take vectors rather than single numbers. For example, let's find the $x_1$ and $x_2$ which minimize the function $g(\\mathbf{x}) = x_1^2 + x_2^2$ ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting at [-1 -1] found minimum at [-7.13245452e-09 -7.13245452e-09]\n",
      "Starting at [-1 -1] found minimum at [-1.07505143e-08 -1.07505143e-08]\n"
     ]
    }
   ],
   "source": [
    "def g(x):\n",
    "    return (x**2).sum()\n",
    "x0 = numpy.array([-1, -1])\n",
    "x1 = numpy.array([1,1])\n",
    "print(\"Starting at {} found minimum at {}\".format(x0, fmin_bfgs(g, x0=x0, disp=False)))\n",
    "print(\"Starting at {} found minimum at {}\".format(x0, fmin_bfgs(g, x0=x1, disp=False)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4.2\n",
    "\n",
    "In this exercise we will regress the fourth feature of the iris dataset agains the first three, using error function minimization. First prepare the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "iris = load_iris()\n",
    "X_train, X_val, y_train, y_val = train_test_split(iris.data[:,:3], \n",
    "                                                  iris.data[:,3], \n",
    "                                                  test_size=1/3, \n",
    "                                                  random_state=666)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now define the error function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error(wb):\n",
    "    '''Returns the error as a function of intercept and coefficients'''\n",
    "    # We'll get the intercept as the fist element of wb, and the coefficients as the rest\n",
    "    b = wb[0]\n",
    "    w = wb[1:]\n",
    "    # complete the function by returning the sum squared error \n",
    "    # .........................................\n",
    "    return numpy.sum((X_train.dot(w)+b - y_train)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a starting point for the intercept and coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First item is the intercept b, the rest the coefficients w\n",
    "wb0 = numpy.array([0.0, 0.0, 0.0, 0.0])\n",
    "#................................"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to find the values which minimize the error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 2.860360\n",
      "         Iterations: 5\n",
      "         Function evaluations: 54\n",
      "         Gradient evaluations: 9\n",
      "[-0.27899144 -0.22846333  0.2540934   0.53891498]\n"
     ]
    }
   ],
   "source": [
    "wb_min = fmin_bfgs(error, x0=wb0)\n",
    "print(wb_min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5.3\n",
    "\n",
    "Let's check how well these parameters do on validation data, in terms of mean absolute error and r-squared. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.167757997204\n",
      "0.914864382525\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "# ..........................................\n",
    "\n",
    "y_val_pred = X_val.dot(wb_min[1:])+wb_min[0]\n",
    "print(mean_absolute_error(y_val, y_val_pred))\n",
    "print(r2_score(y_val, y_val_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4.4\n",
    "Let us compare these results with the classic implementation of linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.278995856785 [-0.22846195  0.25409292  0.53891441]\n",
      "0.167758010212\n",
      "0.914864399764\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "#.........................\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(model.intercept_, model.coef_)\n",
    "# \n",
    "print(mean_absolute_error(y_val, model.predict(X_val)))\n",
    "print(r2_score(y_val, model.predict(X_val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent\n",
    "\n",
    "Scikit learn provides two classes `SGDRegressor` and `SGDClassifier` which use stochastic gradient descent to carry out linear regression and classification respectively.\n",
    "\n",
    "These models are especially useful in these situations:\n",
    "- with very large datasets\n",
    "- with streaming data (they support online learning)\n",
    "- with datasets with sparse features\n",
    "\n",
    "SGD is sensitive to learning rate and the scale of the features. It's advisable to z-score the features, and to tune the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the dataset of 50,000 songs. The prediction task is to guess the year the song was made based on 90 timbre features extracted from the audio. The year is in the first column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "songs = numpy.load(\"songs50k.npy\")\n",
    "X_train, X_val, y_train, y_val = train_test_split(songs[:,1:], songs[:,0], test_size=1/3, random_state=666)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4.5\n",
    "\n",
    "Train and evaluate the SGD classifier on the songs dataset. Take the following steps:\n",
    "- z-score the training and validation features\n",
    "- find good settings for learning rate type and learning rate initial value.\n",
    "  - r-squared on validation data \n",
    "  - mean absolute error on validation data  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('constant', 1e-06, 1691.6939047040719, -25740.804940849972)\n",
      "('constant', 1e-05, 377.32239506868206, -1280.4988745664509)\n",
      "('constant', 0.0001, 6.7552439908788076, 0.20507505576959972)\n",
      "('constant', 0.001, 7.0750764405253479, 0.15984736302556612)\n",
      "('constant', 0.01, 90158067960.919235, -1.5148066821158011e+20)\n",
      "('constant', 0.1, 3375831007998.2964, -2.094275424205043e+23)\n",
      "('constant', 1, 36922938309950.508, -2.4835485588743885e+25)\n",
      "('optimal', 1e-06, 2175674618252.7292, -9.1619072805100201e+22)\n",
      "('optimal', 1e-05, 2175674618252.7292, -9.1619072805100201e+22)\n",
      "('optimal', 0.0001, 2175674618252.7292, -9.1619072805100201e+22)\n",
      "('optimal', 0.001, 2175674618252.7292, -9.1619072805100201e+22)\n",
      "('optimal', 0.01, 2175674618252.7292, -9.1619072805100201e+22)\n",
      "('optimal', 0.1, 2175674618252.7292, -9.1619072805100201e+22)\n",
      "('optimal', 1, 2175674618252.7292, -9.1619072805100201e+22)\n",
      "('invscaling', 1e-06, 1976.6342530096438, -35142.481969525528)\n",
      "('invscaling', 1e-05, 1790.3683811622066, -28831.300535871447)\n",
      "('invscaling', 0.0001, 665.28895242770341, -3981.0546188332687)\n",
      "('invscaling', 0.001, 6.9122748681064143, 0.17634949293650815)\n",
      "('invscaling', 0.01, 6.8830944511310097, 0.19277270571818739)\n",
      "('invscaling', 0.1, 9.8895674835951617, -0.70301046371337628)\n",
      "('invscaling', 1, 1500735465192.8347, -4.3048509040977104e+22)\n",
      "Best settings according to MAE ('constant', 0.0001, 6.7552439908788076, 0.20507505576959972)\n",
      "Best settings according to R2 ('constant', 0.0001, 6.7552439908788076, 0.20507505576959972)\n"
     ]
    }
   ],
   "source": [
    "# .........................................\n",
    "scaler = StandardScaler()\n",
    "X_train_z = scaler.fit_transform(X_train)\n",
    "X_val_z  =scaler.fit_transform(X_val)\n",
    "\n",
    "lr = [ 10 ** x for x in range(-6,1)]\n",
    "\n",
    "settings = []\n",
    "\n",
    "for learning_rate in ['constant', 'optimal', 'invscaling']:\n",
    "    for eta0 in lr:\n",
    "        model = SGDRegressor(learning_rate=learning_rate, eta0=eta0, random_state=666)\n",
    "        model.fit(X_train_z, y_train)\n",
    "        mae = mean_absolute_error(y_val, model.predict(X_val_z))\n",
    "        r2 =  r2_score(y_val, model.predict(X_val_z))\n",
    "        settings.append((learning_rate, eta0, mae, r2))\n",
    "        print(settings[-1])\n",
    "best_mae = min(settings, key=lambda x: x[-2])\n",
    "best_r2 =  max(settings, key=lambda x: x[-1])\n",
    "print(\"Best settings according to MAE {}\".format(best_mae))\n",
    "print(\"Best settings according to R2 {}\".format(best_r2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4.6\n",
    "\n",
    "By default SGDRegressor tries to minimize the standard linear regression error function, that is sum of squared error. However this can be changed, via the `loss=` parameter. When `loss='squared_loss'`, sum of squared errors will be used. Other error functions available include [Huber loss](https://en.wikipedia.org/wiki/Huber_loss) (`loss='huber'`). Compared to squared loss, huber focuses less on outliers.\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/c/cc/Huber_loss.svg/600px-Huber_loss.svg.png)\n",
    "\n",
    "Repeat the steps from the previous exercise, but include the tuning of the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDRegressor'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('constant', 1e-06, 'squared_loss', 1691.6939047040719, -25740.804940849972)\n",
      "('constant', 1e-05, 'squared_loss', 377.32239506868206, -1280.498874566451)\n",
      "('constant', 0.0001, 'squared_loss', 6.755243990878808, 0.20507505576959972)\n",
      "('constant', 0.001, 'squared_loss', 7.075076440525348, 0.15984736302556612)\n",
      "('constant', 0.01, 'squared_loss', 90158067960.91924, -1.514806682115801e+20)\n",
      "('constant', 0.1, 'squared_loss', 3375831007998.2964, -2.094275424205043e+23)\n",
      "('constant', 1, 'squared_loss', 36922938309950.51, -2.4835485588743885e+25)\n",
      "('constant', 1e-06, 'huber', 1998.4742436817962, -35923.35821099075)\n",
      "('constant', 1e-05, 'huber', 1998.3242451817932, -35917.96584966415)\n",
      "('constant', 0.0001, 'huber', 1996.8242601817703, -35864.064497347186)\n",
      "('constant', 0.001, 'huber', 1981.824410182078, -35327.27706924383)\n",
      "('constant', 0.01, 'huber', 1831.8259102310358, -30182.01248012661)\n",
      "('constant', 0.1, 'huber', 331.84091416308604, -990.5670872853742)\n",
      "('constant', 1, 'huber', 9.832523470409532, -0.6197692785961364)\n",
      "('optimal', 1e-06, 'squared_loss', 2175674618252.7292, -9.16190728051002e+22)\n",
      "('optimal', 1e-05, 'squared_loss', 2175674618252.7292, -9.16190728051002e+22)\n",
      "('optimal', 0.0001, 'squared_loss', 2175674618252.7292, -9.16190728051002e+22)\n",
      "('optimal', 0.001, 'squared_loss', 2175674618252.7292, -9.16190728051002e+22)\n",
      "('optimal', 0.01, 'squared_loss', 2175674618252.7292, -9.16190728051002e+22)\n",
      "('optimal', 0.1, 'squared_loss', 2175674618252.7292, -9.16190728051002e+22)\n",
      "('optimal', 1, 'squared_loss', 2175674618252.7292, -9.16190728051002e+22)\n",
      "('optimal', 1e-06, 'huber', 6.500724991366285, 0.15009706846473447)\n",
      "('optimal', 1e-05, 'huber', 6.500724991366285, 0.15009706846473447)\n",
      "('optimal', 0.0001, 'huber', 6.500724991366285, 0.15009706846473447)\n",
      "('optimal', 0.001, 'huber', 6.500724991366285, 0.15009706846473447)\n",
      "('optimal', 0.01, 'huber', 6.500724991366285, 0.15009706846473447)\n",
      "('optimal', 0.1, 'huber', 6.500724991366285, 0.15009706846473447)\n",
      "('optimal', 1, 'huber', 6.500724991366285, 0.15009706846473447)\n",
      "('invscaling', 1e-06, 'squared_loss', 1976.6342530096438, -35142.48196952553)\n",
      "('invscaling', 1e-05, 'squared_loss', 1790.3683811622066, -28831.300535871447)\n",
      "('invscaling', 0.0001, 'squared_loss', 665.2889524277034, -3981.0546188332687)\n",
      "('invscaling', 0.001, 'squared_loss', 6.912274868106414, 0.17634949293650815)\n",
      "('invscaling', 0.01, 'squared_loss', 6.88309445113101, 0.1927727057181874)\n",
      "('invscaling', 0.1, 'squared_loss', 9.889567483595162, -0.7030104637133763)\n",
      "('invscaling', 1, 'squared_loss', 1500735465192.8347, -4.30485090409771e+22)\n",
      "('invscaling', 1e-06, 'huber', 1998.4898104395422, -35923.91785062578)\n",
      "('invscaling', 1e-05, 'huber', 1998.4799127592553, -35923.56202213557)\n",
      "('invscaling', 0.0001, 'huber', 1998.3809359563868, -35920.00383417261)\n",
      "('invscaling', 0.001, 'huber', 1997.3911679277207, -35884.43164845599)\n",
      "('invscaling', 0.01, 'huber', 1987.4934876433915, -35529.67917850278)\n",
      "('invscaling', 0.1, 'huber', 1888.5166850203861, -32079.089594824043)\n",
      "('invscaling', 1, 'huber', 898.7486720050475, -7265.526262708986)\n",
      "Best settings according to MAE ('optimal', 1e-06, 'huber', 6.500724991366285, 0.15009706846473447)\n",
      "Best settings according to R2 ('constant', 0.0001, 'squared_loss', 6.755243990878808, 0.20507505576959972)\n"
     ]
    }
   ],
   "source": [
    "#................................................\n",
    "scaler = StandardScaler()\n",
    "X_train_z = scaler.fit_transform(X_train)\n",
    "X_val_z  =scaler.fit_transform(X_val)\n",
    "\n",
    "lr = [ 10 ** x for x in range(-6,1)]\n",
    "\n",
    "settings = []\n",
    "\n",
    "for learning_rate in ['constant', 'optimal', 'invscaling']:\n",
    "  for loss in ['squared_loss', 'huber']:\n",
    "    for eta0 in lr:\n",
    "        model = SGDRegressor(learning_rate=learning_rate, eta0=eta0, loss=loss, random_state=666)\n",
    "        model.fit(X_train_z, y_train)\n",
    "        mae = mean_absolute_error(y_val, model.predict(X_val_z))\n",
    "        r2 =  r2_score(y_val, model.predict(X_val_z))\n",
    "        settings.append((learning_rate, eta0, loss, mae, r2))\n",
    "        print(settings[-1])\n",
    "best_mae = min(settings, key=lambda x: x[-2])\n",
    "best_r2 =  max(settings, key=lambda x: x[-1])\n",
    "print(\"Best settings according to MAE {}\".format(best_mae))\n",
    "print(\"Best settings according to R2 {}\".format(best_r2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
