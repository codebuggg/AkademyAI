{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to NLTK - part 1\n",
    "\n",
    "This notebook will give you an introduction to NLTK, and how to use it for Natural Language Processing.\n",
    "\n",
    "Before executing this notebook, make sure you have NLTK installed, and that you have downloaded the necessary resources.\n",
    "\n",
    "**Installing NLTK:**\n",
    "- with conda (from the shell): `conda install nltk`\n",
    "- with pip (from the shell): `pip install nltk`\n",
    "\n",
    "**Downloading the necessary resournces:**\n",
    "- To download only some specific resources, do the following (from the shell):<br>\n",
    "    `> import nltk`<br>\n",
    "    `> nltk.download('gutenberg')`<br>\n",
    "    `> nltk.download('stopwords')`<br>\n",
    "    `> nltk.download('punkt')`<br>\n",
    "    `> nltk.download('averaged_perceptron_tagger')`<br>\n",
    "    `> nltk.download('wordnet')`<br>\n",
    "\n",
    "- To download all the non-default resources, do the following (from the shell):<br>\n",
    "    `> import nltk`<br>\n",
    "    `> nltk.download()`<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus reader\n",
    "\n",
    "#### Example using the Gutenberg corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "# list of files\n",
    "nltk.download('gutenberg')\n",
    "#nltk.download()\n",
    "nltk.corpus.gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8354"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load a file\n",
    "cp = nltk.corpus.gutenberg.words('blake-poems.txt')\n",
    "len(cp) # length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'same', 'again', ',', 'While', 'he', 'wept', 'with', 'joy']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cp[100:109] # subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1820"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(cp)) # set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Poems',\n",
       " 'William',\n",
       " 'Blake',\n",
       " '1789',\n",
       " 'SONGS',\n",
       " 'INNOCENCE',\n",
       " 'AND',\n",
       " 'EXPERIENCE',\n",
       " 'and',\n",
       " 'THE',\n",
       " 'BOOK',\n",
       " 'THEL',\n",
       " 'SONGS',\n",
       " 'INNOCENCE',\n",
       " 'INTRODUCTION',\n",
       " 'Piping',\n",
       " 'down',\n",
       " 'the',\n",
       " 'valleys',\n",
       " 'wild',\n",
       " 'Piping',\n",
       " 'songs',\n",
       " 'pleasant',\n",
       " 'glee',\n",
       " 'cloud',\n",
       " 'saw',\n",
       " 'child',\n",
       " 'And',\n",
       " 'laughing',\n",
       " 'said']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list of words with more than 2 chars\n",
    "lst = [w for w in cp if len(w)>2]\n",
    "lst[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('!\"--', 4), ('\\',\"', 3), (',--', 3), ('1780', 4), ('1789', 4)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tuples with words lowered and length\n",
    "tup = [(w.lower(), len(w)) for w in lst]\n",
    "sorted(tup)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('the', 351)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# frequencies of words\n",
    "freqs = {w:lst.count(w) for w in set(lst)}\n",
    "kmax = max(freqs, key=lambda k: freqs[k])\n",
    "kmax, freqs[kmax]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopwords reader\n",
    "\n",
    "The stopwords reader provides the list of stopwords of a specific language. Stopwords are words that do not have individual meaning (pronouns, determiners, auxiliary verbs, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "sw = set(stopwords.words('english'))\n",
    "len(sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'the' in sw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: develop a jupyter notebook that shows the 25 non-stopwords with more number of occurrences in the file 'blake-poems.txt' of Gutenberg corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Text\n",
    "\n",
    "#### Consulting occurences of words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "from nltk.text import Text\n",
    "\n",
    "crp = Text(gutenberg.words('blake-poems.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# counting words\n",
    "crp.count('love')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crp.count('Love')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 25 of 29 matches:\n",
      "at we may learn to bear the beams of love And these black bodies and this sunb\n",
      "ing , ' Come out from the grove , my love and care And round my golden tent li\n",
      ", And be like him , and he will then love me . THE BLOSSOM Merry , merry sparr\n",
      " IMAGE To Mercy , Pity , Peace , and Love , All pray in their distress , And t\n",
      "ess . For Mercy , Pity , Peace , and Love , Is God our Father dear ; And Mercy\n",
      "ear ; And Mercy , Pity , Peace , and Love , Is man , his child and care . For \n",
      "uman heart Pity , a human face ; And Love , the human form divine ; And Peace \n",
      "s , Prays to the human form divine : Love , Mercy , Pity , Peace . And all mus\n",
      " Mercy , Pity , Peace . And all must love the human form , In heathen , Turk ,\n",
      "then , Turk , or Jew . Where Mercy , Love , and Pity dwell , There God is dwel\n",
      "sh , vain , Eternal bane , That free love with bondage bound .\" THE CLOD AND T\n",
      "e bound .\" THE CLOD AND THE PEBBLE \" Love seeketh not itself to please , Nor f\n",
      "ok Warbled out these metres meet : \" Love seeketh only Self to please , To bin\n",
      "Of crimson joy , And his dark secret love Does thy life destroy . THE FLY Litt\n",
      "horn : While the Lily white shall in love delight , Nor a thorn nor a threat s\n",
      "in her beauty bright . THE GARDEN OF LOVE I laid me down upon a bank , Where L\n",
      "E I laid me down upon a bank , Where Love lay sleeping ; I heard among the rus\n",
      "the chaste . I went to the Garden of Love , And saw what I never had seen ; A \n",
      " door ; So I turned to the Garden of Love That so many sweet flowers bore . An\n",
      "to know . \" And , father , how can I love you Or any of my brothers more ? I l\n",
      "e you Or any of my brothers more ? I love you like the little bird That picks \n",
      "nt page , Know that in a former time Love , sweet love , was thought a crime .\n",
      "w that in a former time Love , sweet love , was thought a crime . In the age o\n",
      " of my hoary hair !\" THE SCHOOLBOY I love to rise on a summer morn , When bird\n",
      "n Wisdom be put in a silver rod ? Or Love in a golden bowl ? THE BOOK of THEL \n"
     ]
    }
   ],
   "source": [
    "# consulting context in a corpus\n",
    "crp.concordance('love')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Consulting contexts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "youth went by innocence experience he laughing me joy all sight sweet\n",
      "peace thrush white away say joys round black\n"
     ]
    }
   ],
   "source": [
    "# words in the same context\n",
    "crp.similar('love')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No common contexts were found\n"
     ]
    }
   ],
   "source": [
    "# words in both context\n",
    "crp.common_contexts(['love', 'laugh'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dispersion plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "crp.dispersion_plot(['love', 'laugh'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: remake the same steps in the example above but with the file `austen-sense.txt` of Gutenberg corpus (in a new notebook). Compare the results with those in the example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plain Text example\n",
    "\n",
    "Loading corpus from a text file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23714"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crp = nltk.corpus.PlaintextCorpusReader('data', '.*\\.txt').words()\n",
    "len(crp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'Project',\n",
       " 'Gutenberg',\n",
       " 'EBook',\n",
       " 'of',\n",
       " 'Alice',\n",
       " 'in',\n",
       " 'Wonderland',\n",
       " ',',\n",
       " 'by']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crp[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Web example\n",
    "\n",
    "Fetching web data as string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Project Gutenberg EBook of Alice in Wonderland, by'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib.request\n",
    "\n",
    "url = 'http://www.gutenberg.org/cache/epub/35688/pg35688.txt'\n",
    "\n",
    "with urllib.request.urlopen(url) as response:\n",
    "    dt = response.read().decode('utf8')\n",
    "\n",
    "dt[1:55]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Syntax analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentence splitting & tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Men want children.', 'They get relaxed with kids.']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sentence splitting\n",
    "source = 'Men want children. They get relaxed with kids.'\n",
    "sentences = nltk.sent_tokenize(source)\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Men', 'want', 'children', '.'],\n",
       " ['They', 'get', 'relaxed', 'with', 'kids', '.']]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenizer\n",
    "words_sentences = [nltk.word_tokenize(s) for s in sentences]\n",
    "words_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['men', 'want', 'children', 'they', 'get', 'relaxed', 'with', 'kids']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# removing punctuation and normalizing the words to lower\n",
    "words = [word.lower() for word in nltk.word_tokenize(source) if word.isalpha()]\n",
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Spelling corrector\n",
    "\n",
    "**Levenshtein edit distance:** the edit distance is the number of characters that need to be substituted, inserted, or deleted, to transform the first string to the second one.\n",
    "\n",
    "Example:\n",
    "“rain” → “sain” → “shin” → “shine” <br>\n",
    "It is needed at least 3 steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.metrics.distance import edit_distance\n",
    "edit_distance('something', 'soothing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first approach to perform spelling correction needs a list of words such as the attached file wordsEn.txt: (http://www-01.sil.org/linguistics/wordlists/english/)\n",
    "\n",
    "The input of the process should a word to be corrected.\n",
    "The output of the process should be the same word if it is included in the previous word list; or the word in the list with minimum edit distance to the input word if it do not belong to the list.\n",
    "\n",
    "**Steps:**\n",
    "1. Read the word list from the attached file\n",
    "2. Implement this basic approach inside a function\n",
    "3. Use the approach to correct the words: 'something', 'soemthing' and some other of your choose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional exercise: Towards a real approach\n",
    "\n",
    "In order to implement a real system, some issues should be studied:\n",
    "- the edit distance is a high time consuming function.\n",
    "- In an interactive solution the output should be those words with minimum distance. E.g.: sat → set, sit, sad\n",
    "- For an automatic solution we need some kind of language model. Resource: _Birkbeck spelling error corpus_ from the _Oxford Text Archive_.\n",
    "- New approaches use context information\n",
    "\n",
    "See the Peter Norvig’s article _How to Write a Spelling Corrector_ (http://www.norvig.com/spell-correct.html) for delving into the topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: SMS spam classifier\n",
    "\n",
    "Spam filtering is a type of text categorization, such as language identification and sentiment analysis.\n",
    "\n",
    "How we represent text by means of features?\n",
    "- _bag of words_: boolean codifying the presence of the indexed words (`nltk.FreqDist`)\n",
    "- _term frequency (tf)_: frequency of the indexed word in the sentence (`nltk.text.tf`)\n",
    "- _term-frequency times inverse document-frequency (tf/idf)_: as above re-weighting to avoid effect of too common words such as english word _the_ (`nltk.text.tf_idf`)\n",
    "\n",
    "#### SMS Spam Collection Data Set\n",
    "\n",
    "source: UCI repository\n",
    "https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection\n",
    "- 5574 examples\n",
    "- 2 classes: ham and spam\n",
    "- Example: `ham    What you doing?how are you?`\n",
    "\n",
    "**Experiment design:**\n",
    "- single validation (50% - 50%)\n",
    "- randomly shuffle\n",
    "- punctuation removed\n",
    "- strings lowered\n",
    "\n",
    "**Steps:**\n",
    "1. Prepare the train and test dataset following the experiment design listed above\n",
    "2. Using the bag of words, convert the train and test set into a vector of occurrences. This will be useful in order to use it as an input for a clasifier.\n",
    "3. Train and test a simple kNN classifier\n",
    "4. Compute the confusion matrix and analyze the results\n",
    "5. **Optional:** Perform additional experiments with different experiment design. Some suggestions:\n",
    "    - Different weight for the single validation\n",
    "    - k-Fold cross-validation\n",
    "    - Different text processing (punctuation vs. no punctuation, strings lowered vs. not lowered)\n",
    "    - Dfferent parameters for the k-NN classifier (k, distance)\n",
    "    - Different algorithms for the classifier (e.g. SVM)\n",
    "    \n",
    "**Optional:** Extend the solution to the use of lemmas and other preprocess issues (after tomorrow's class)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bag of words:\n",
    "_N.B.: the `train`and `test` variables are not defined in the code below, and should be defined by you following the experiment design contrains listed above._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#Creating a CountVectorizer()\n",
    "cv = CountVectorizer()\n",
    "\n",
    "#Here we create a matrix of the words observed (this will be our vocabulary)\n",
    "Xtrn = cv.fit_transform([' '.join(ex[1]) for ex in train])\n",
    "#For the test set, we will create the occurrence matrix using only the vocabulary seen in the previous line\n",
    "Xtst = cv.transform([' '.join(ex[1]) for ex in test])\n",
    "#Training labels\n",
    "Ytrn = [ex[0] for ex in train]\n",
    "#Test labels\n",
    "Ytst = [ex[0] for ex in test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### k Nearest Neighbours:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "clf = KNeighborsClassifier(1)\n",
    "clf.fit(Xtrn,Ytrn)\n",
    "preds = clf.predict(Xtst).tolist()\n",
    "round(accuracy(Ytst,preds),3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.metrics import ConfusionMatrix\n",
    "\n",
    "print(ConfusionMatrix(Ytst,preds).pretty_format())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
